print("page number:")
print(page_number);
print("j:")
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 2){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
}
print(i)
}
request
request = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url&fq=source:("The New York Times")&q=Arts&page=1&api-key=6a2953191973c2ecbd66dace80be613f:0:68912737';
response <- fromJSON(file=request);
request
request = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url&fq=source:('The New York Times')&q=Arts&page=1&api-key=6a2953191973c2ecbd66dace80be613f:0:68912737';
request = "http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url&fq=source:('The New York Times')&q=Arts&page=1&api-key=6a2953191973c2ecbd66dace80be613f:0:68912737";
response <- fromJSON(file=request);
request = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url&fq=source:("The New York Times")&q=Arts&page=1&api-key=6a2953191973c2ecbd66dace80be613f:0:68912737';
dQuote(request);
sQuote(request);
request = http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url&fq=source:("The New York Times")&q=Arts&page=1&api-key=6a2953191973c2ecbd66dace80be613f:0:68912737;
request = http:\/\/api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url&fq=source:("The New York Times")&q=Arts&page=1&api-key=6a2953191973c2ecbd66dace80be613f:0:68912737;
request = "http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url&fq=source:(\"The New York Times\")&q=Arts&page=1&api-key=6a2953191973c2ecbd66dace80be613f:0:68912737";
request
help(read_html)
help(fromJSON)
response <- fromJSON(file=request, unexpected.escape = "skip");
response <- fromJSON(file=request, unexpected.escape = "keep");
response <- fromJSON(file=request, unexpected.escape = "error");
response
request
response <- fromJSON(file=request, unexpected.escape = "error");
response <- fromJSON(json_str=request, unexpected.escape = "error");
response <- fromJSON(file=request, unexpected.escape = "error");
URLencode(request)
response <- fromJSON(file=URLEncode(request));
response <- fromJSON(file=URLencode(request));
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= 1){
#fetch 10 results from
request <- paste(base_request, "&q=",sections[i],"&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
print("page number:")
print(page_number);
print("j:")
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 2){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
}
print(i)
}
base_request <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url";
sections <- c("Arts", "Business", "Obituaries", "Sports", "World")
# Fetch 2000 most recent articles from each
# of the above sections
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= 1){
#fetch 10 results from
request <- paste(base_request, "&fq=section_name(\"",sections[i],"\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
print("page number:")
print(page_number);
print("j:")
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 2){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
}
print(i)
}
sections <- c("Arts", "Business", "Obituaries", "Sports", "World")
# Fetch 2000 most recent articles from each
# of the above sections
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=section_name(\"",sections[i],"\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
request
web_url
response$response$docs[[7]]$web_url;
response$response$docs[[7]]$headline;
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=web_url:(\"nytimes\") AND section_name:(\"",sections[i],"\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
response$response$meta
request
base_request <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url";
sections <- c("Arts", "Business", "Obituaries", "Sports", "World")
# Fetch 2000 most recent articles from each
# of the above sections
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=web_url:(\"nytimes\")ANDsection_name:(\"",sections[i],"\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
base_request <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url";
sections <- c("Arts", "Business", "Obituaries", "Sports", "World")
# Fetch 2000 most recent articles from each
# of the above sections
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=web_url:(\"nytimes\")ANDsection_name:(\"",sections[i],"\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=section_name:(\"",sections[i],"\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
request
response
response$response$meta
response$response$docs[[1]]$headline
base_request <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url";
sections <- c("Arts", "Business", "Obituaries", "Sports", "World")
# Fetch 2000 most recent articles from each
# of the above sections
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=section_name:(\"",sections[i],"\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
title
text
web_url
response
response$response$docs[[1]]
response$response$docs[[1]]$web_url;
base_request <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url";
sections <- c("Arts", "Business", "Obituaries", "Sports", "World")
# Fetch 2000 most recent articles from each
# of the above sections
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=news_desk:(\"",sections[i],"\") AND source(\"The New York Times\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
request
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=section_name:(\"",sections[i],"\") AND document_type:(\"article\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
web_url
response$status
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=section_name:(\"",sections[i],"\") AND document_type:(\"article\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10 && length(articles[[i]]) <=2000){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
if(response$response$status == "OK"){
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
}
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(length(articles[[i]]) >=2000){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
response$status
response$status
response$response$meta;
help(read_html)
text
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=section_name:(\"",sections[i],"\") AND document_type:(\"article\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10 && length(articles[[i]]) <=2000){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
if(response$status == "OK"){
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
}
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(length(articles[[i]]) >=2000){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
source("script.r")
setwd("Development/483/Project")
source("script.r")
load("World.rdata")
load("Sports.rdata")
load("Obituaries.rdata")
load("Business.rdata")
load("Business.rdata")
load("Arts.rdata")
articles.list <- list (Arts[1:1000], Sports[1:1000], World[1:1000], Business[1:1000], Obituaries[1:1000])
save(articles.list, file="articles.rdata")
frequency <- calculateWordFrequency(getTextFromArticleList(articles.list));
View(as.matrix(frequency))
dictionary <- prepDictionary(frequency)
View(as.matrix(dictionary))
dict <- dictionary[13:]
dict <- dictionary[13:18355]
View(as.matrix(dict))
dict <- dictionary[13:18355]
dict <- dictionary[14:18355]
dictionary <- dict
save(dictionary, file="dictionary.rdata")
