#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
request
web_url
response$response$docs[[7]]$web_url;
response$response$docs[[7]]$headline;
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=web_url:(\"nytimes\") AND section_name:(\"",sections[i],"\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
response$response$meta
request
base_request <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url";
sections <- c("Arts", "Business", "Obituaries", "Sports", "World")
# Fetch 2000 most recent articles from each
# of the above sections
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=web_url:(\"nytimes\")ANDsection_name:(\"",sections[i],"\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
base_request <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url";
sections <- c("Arts", "Business", "Obituaries", "Sports", "World")
# Fetch 2000 most recent articles from each
# of the above sections
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=web_url:(\"nytimes\")ANDsection_name:(\"",sections[i],"\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=section_name:(\"",sections[i],"\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
request
response
response$response$meta
response$response$docs[[1]]$headline
base_request <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url";
sections <- c("Arts", "Business", "Obituaries", "Sports", "World")
# Fetch 2000 most recent articles from each
# of the above sections
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=section_name:(\"",sections[i],"\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
title
text
web_url
response
response$response$docs[[1]]
response$response$docs[[1]]$web_url;
base_request <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?fl=headline,web_url";
sections <- c("Arts", "Business", "Obituaries", "Sports", "World")
# Fetch 2000 most recent articles from each
# of the above sections
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=news_desk:(\"",sections[i],"\") AND source(\"The New York Times\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
request
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=section_name:(\"",sections[i],"\") AND document_type:(\"article\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(page_number >= 10){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
web_url
response$status
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=section_name:(\"",sections[i],"\") AND document_type:(\"article\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10 && length(articles[[i]]) <=2000){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
if(response$response$status == "OK"){
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
}
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(length(articles[[i]]) >=2000){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
response$status
response$status
response$response$meta;
help(read_html)
text
#loop over
i <- 1;
page_number <- 1;
articles <- list()
articles[[i]] <- list()
while(i <= length(sections)){
#fetch 10 results from
request <- paste(base_request, "&fq=section_name:(\"",sections[i],"\") AND document_type:(\"article\")&page=", page_number, "&api-key=", api_key, sep="");
request <- URLencode(request);
response <- fromJSON(file=request);
j <- 1;
while(j <= 10 && length(articles[[i]]) <=2000){
print(j);
web_url <- response$response$docs[[j]]$web_url;
title <- response$response$docs[[j]]$headline$main;
if(response$status == "OK"){
#now get the article text
web_page <- read_html(web_url);
#now extract text from article
text <- web_page %>% html_nodes(".article-body")%>%html_text();
articles[[i]] <- c(articles[[i]], data.frame(Title=title, Text=text));
}
j <- j + 1;
}
#get url from data
page_number <- page_number + 1
#print(request);
if(length(articles[[i]]) >=2000){
i <- i + 1;
articles[[i]] <- list();
page_number <- 0;
print(i);
}
}
?data.table
??data.table
?table
install.packages("data.table")
?data.table
library(data.table)
load("articles.rdata")
setwd("Development/483/Project")
load("articles.rdata")
length(articles[[1]])
length(articles.list[[1]])
length(articles.list[[2]])
length(articles.list[[3]])
length(articles.list[[4]])
length(articles.list[[5]])
articles.list[[4]][[900]]$Text
articles.list <- c(articles.list[1:3], articles.list[[4]])
source("script.r")
load("spm1.rdata")
load("spm2.rdata")
load("spm3.rdata")
load("spm5.rdata")
spm.list <- list(spm.1, spm.2, spm.3, spm.5)
w <- trainClassifier(spm.list, 2, 4)
load("test1.rdata")
load("test2.rdata")
load("test3.rdata")
load("test5.rdata")
table <- test.sparse(spm.list, w, dictionary, c(sections[1:3], sections[[5]]))
load("dictionary.rdata")
source("script.r")
table <- test.sparse(spm.list, w, dictionary, c(sections[1:3], sections[[5]]))
table <- test.sparse(list(test1, test2, test3, test5), w, dictionary, c(sections[1:3], sections[[5]]))
load("test3.rdata")
load("test3.rdata")
table <- test.sparse(list(test1, test2, test3, test5), w, dictionary, c(sections[1:3], sections[[5]]))
View(as.matrix(table))
sum(table)
(10001+872+889)/4004
(1001+872+889)/4004
c(sections[1:3], sections[[5]])
w[[4]]$Weight[[1]]
w[[4]]$Weight[[2]]
printMostImportant(c(sections[1:3], sections[[5]]), w, dictionary)
x<- loadFromFile("Business.txt")
length(x)
x<- loadFromFile("Business.txt")
length(x)
x[[1000]]$Text
x[[1000]]$Title
x[!is.na(x$Text)]
x<- loadFromFile("Business.txt")
length(x)
w <- trainClassifier(spm.list, 2, 3)
source("script.r")
table <- test.sparse(list(test1, test2, test3, test5), w, dictionary, c(sections[1:3], sections[[5]]))
load("test1.rdata")
load("test3.rdata")
source("script.r")
table <- test.sparse(list(test1, test2, test3, test5), w, dictionary, c(sections[1:3], sections[[5]]))
View(as.table(table))
View(as.matrix(table))
View(as.matrix(table))
View(as.matrix(w[[4]]$Weight))
View(spm.5)
sum(spm.5[,1])
sum(as.numeric(spm.5[,1]))
sum(as.numeric(spm.5[,2]))
sum(as.numeric(spm.5[,3]))
sum(as.numeric(spm.5[,4]))
sum(as.numeric(spm.5[,5]))
sum(as.numeric(spm.5[,6]))
sum(as.numeric(spm.5[,7]))
sum(as.numeric(spm.5[,8]))
articles.list[[5]][[8]]$Text
length(articles.list)
x <- loadFromFile("Business.txt")
length(x)
length(x)
length(x)
x <- loadFromFile("Business.txt")
length(x)
View(prepareSparseDocumentMatrix)
View(prepareSparseDocumentMatrix)
source("script.r")
source("script.r")
w <- trainClassifier(spm.list, 2, 3)
table <- test.sparse(list(test1, test2, test3, test5), w, dictionary, c(sections[1:3], sections[[5]]))
View(as.matrix(table))
sum(spm.list[[4]])
sum(as.numeric(spm.list[[4]]))
sum(as.numeric(spm.list[[3]]))
sum(as.numeric(spm.list[[2]]))
load("Obituaries.rdata")
length(Obituaries)
x <- calculateWordFrequency(getTextFromArticleList(list(Obituaries[1:1000])))
View(as.matrix(x))
z <- lapply(X=Obituaries, FUN= function(x){if(is.na(x$Text){return (NA)} return (x))})
z <- lapply(X=Obituaries, FUN= function(x){if(is.na(x$Text)){return (NA);} return (x))})
z <- lapply(X=Obituaries, FUN= function(x){if(is.na(x$Text)){return (NA)} return (x)})
z <- lapply(X=Obituaries, FUN= function(x){
if(is.na(x$Text)) return (NA)
else return (x)
})
length(z[!is.na(z)])
length(z[is.na(z)])
load("spm.4")
load("spm4.rdata")
spm.list <- list (spm.1, spm.2, spm.3, spm.4)
w <- trainClassifier(spm.list, 2, 3)
View(getArticlesFromUrlFile)
load("test4.rdata")
table <- test.sparse(list(test1, test2, test3), w, dictionary, sections[1:3])
table <- test.sparse(list(test1, test2, test3), w, dictionary, sections[1:4])
w <- trainClassifier(spm.list[1:3], 2, 3)
table <- test.sparse(list(test1, test2, test3), w, dictionary, sections[1:3])
View(as.matrix(table))
(1001 + 872+889)/3000
load("test4.rdata")
w <- trainClassifier(spm.list, 2, 3)
table <- test.sparse(list(test1, test2, test3, test4), w, dictionary, sections)
source("script.r")
sumConfTable(table)
View(as.matrix(table))
table <- test.sparse(list(test1, test2, test3, test4), w, dictionary, sections[1:4])
sumConfTable(table)
length(table)
dim(table)
source("script.r")
sumConfTable(table)
sumConfTable(table)/4000
x <- testAlphaBeta(list(test1, test2, test3, test4), spm.list, dictionary, sections)
x
x[[1]]
source("script.r")
x <- testAlphaBeta(list(test1, test2, test3, test4), spm.list, dictionary, sections)
x
x <- testAlphaBeta(list(test1, test2, test3, test4), spm.list, dictionary, sections[1:4])
x[[1]]
x[[2]]
w <- trainClassifier(spm.list, 2, 3)
table <- test.sparse(list(test1, test2, test3, test4), w, dictionary, sections[1:4])
sumConfTable(table)
source("script.r")
table <- test.sparse(list(test1, test2, test3, test4), w, dictionary, sections[1:4])
w <- trainClassifier(spm.list, 2, 3)
table <- test.sparse(list(test1, test2, test3, test4), w, dictionary, sections[1:4])
sumConfTable(table)
View(as.matrix(w[[1]]$Weight))
View(as.matrix(w[[2]]$Weight))
source("script.r")
w <- trainClassifier(spm.list, 2, 3)
View(as.matrix(w[[1]]$Weight))
table <- test.sparse(list(test1, test2, test3, test4), w, dictionary, sections[1:4])
sumConfTable(table)
View(as.matrix(table))
source("script.r")
w <- trainClassifier(spm.list, 2, 3)
View(as.matrix(w[[1]]$Weight))
View(as.matrix(w[[2]]$Weight))
table <- test.sparse(list(test1, test2, test3, test4), w, dictionary, sections[1:4])
sumConfTable(table)
source("script.r")
w <- trainClassifier(spm.list, 2, 3)
table <- test.sparse(list(test1, test2, test3, test4), w, dictionary, sections[1:4])
table
source("script.r")
w <- trainClassifier(spm.list, 2, 3)
